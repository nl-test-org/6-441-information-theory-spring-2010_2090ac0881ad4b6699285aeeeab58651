---
course_id: 6-441-information-theory-spring-2010
layout: course_section
menu:
  leftnav:
    identifier: ef9c0e21724000a42c5e2febc0b09448
    name: Lecture Notes
    weight: 40
title: Lecture Notes
type: course
uid: ef9c0e21724000a42c5e2febc0b09448

---

| LEC # | TOPICS | LECTURE NOTES |
| --- | --- | --- |
| 1 | Introduction, entropy | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec01)) |
| 2 | Jensen's inequality, data processing theorem, Fanos's inequality | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec02)) |
| 3 | Different types of convergence, asymptotic equipartition property (AEP), typical set, joint typicality | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec03)) |
| 4 | Entropies of stochastic processes | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec04)) |
| 5 | Data compression, Kraft inequality, optimal codes | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec05)) |
| 6 | Huffman codes | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec06)) |
| 7 | Shannon-Fano-Elias codes, Slepian-Wolf | ([PDF 1]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec07a)) ([PDF 2]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec07b)) |
| 8 | Channel capacity, binary symmetric and erasure channels | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec08)) |
| 9 | Maximizing capacity, Blahut-Arimoto | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec09)) |
| 10 | The channel coding theorem | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec10)) |
| 11 | Strong coding theorem, types of errors | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec11)) |
| 12 | Strong coding theorem, error exponents | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec12)) |
| 13 | Fano's inequality and the converse to the coding theorem | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec13)) |
| 14 | Feedback capacity | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec14)) |
| 15 | Joint source channel coding | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec15)) |
| 16 | Differential entropy, maximizing entropy | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec16)) |
| 17 | Additive Gaussian noise channel | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec17)) |
| 18 | Gaussian channels: parallel, colored noise, inter-symbol interference | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec18)) |
| 19 | Gaussian channels with feedback | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec19)) |
| 20 | Multiple access channels | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec20)) |
| 21 | Broadcast channels | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec21)) |
| 22 | Finite state Markov channels | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec22)) |
| 23 | Channel side information, wide-band channels | ([PDF]({{< baseurl >}}/sections/lecture-notes/mit6_441s10_lec23))